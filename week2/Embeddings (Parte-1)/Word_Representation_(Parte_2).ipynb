{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Representation (Parte 2).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "UXYKlaMUihse",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table style=\"width:100%\">\n",
        "  <tr>\n",
        "    <td><center style=\"font-size:500%;\">Representação Distribuida de Palavras (Parte 2)</center></td>\n",
        "    <td><img src=\"https://logodownload.org/wp-content/uploads/2015/02/puc-rio-logo.gif\" width=\"100\"/></td> \n",
        "  </tr>    \n",
        "</table>\n",
        "\n",
        "Msc. Cristian Muñoz V."
      ]
    },
    {
      "metadata": {
        "id": "rA__3tfqVK2p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import csv\n",
        "import gensim\n",
        "from itertools import groupby\n",
        "from gensim.similarities import WmdSimilarity\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "nltk.download('punkt') \n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BGoH0xvSXMfQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_files(filenames):\n",
        "  lines = []\n",
        "  # Lendo todos os documentos de texto\n",
        "  for index,filename in enumerate(filenames):\n",
        "\n",
        "      file = open(filename, 'r', encoding='utf-8')\n",
        "\n",
        "      for line in file:\n",
        "\n",
        "          # Remover espaços em branco \n",
        "          line = line.strip()\n",
        "\n",
        "          # Pula linhas em branco\n",
        "          if len(line) == 0:\n",
        "              continue\n",
        "\n",
        "          lines.append(line)\n",
        "\n",
        "      if index % 10 ==0:\n",
        "          print(\".\",end=\" \")\n",
        "  \n",
        "  print(\"Letirua Completada!\")\n",
        "  \n",
        "  text = \" \".join(lines)\n",
        "  \n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tZme1RalVL3s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filenames =  glob.glob(\"PRH/*\")\n",
        "\n",
        "text = read_files(filenames)\n",
        "text = nltk.word_tokenize(text , language='portuguese')\n",
        "print(\"Tokenizando Completada!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wZZoxmvgXh14",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Agrupamos por texto que se encontra entre 2 ponto.\n",
        "sentences = [list(group) for k, group in groupby(text, lambda x: x == \".\")\\\n",
        "                             if not k]\n",
        "\n",
        "# Remove stopwords numbers and punctuation.\n",
        "\n",
        "def preprocess(sentence):\n",
        "  return [word for word in sentence\\\n",
        "                if word not in stop_words and word.isalpha()]\n",
        "\n",
        "sentences = [preprocess(sentence) for sentence in sentences]\n",
        "\n",
        "# Seleccionamos frases com dimensão entre 3 e 150 palavras.\n",
        "sentences =  [sentence for sentence in sentences\\\n",
        "                        if len(sentence)>3 and len(sentence)<150]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YfKZAAOnYlK_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lens = [len(sentence) for sentence in sentences]\n",
        "avg_len = sum(lens) / float(len(lens))\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist([len(sentence) for sentence in sentences])\n",
        "plt.axvline(avg_len, color='#e41a1c')\n",
        "plt.title('Histograma de cumprimento de frases.')\n",
        "plt.xlabel('tamanho')\n",
        "plt.text(10, 40000, 'mean = %.2f' % avg_len)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xqcj6pq5ZBF-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Treinamos Word2Vec com todos as frases\n",
        "model = Word2Vec(sentences, workers=2, size=50)\n",
        "\n",
        "# Calculamos a similaridade entre um conjunto de frases\n",
        "num_best = 10\n",
        "instance = WmdSimilarity(sentences[:1000], model, num_best=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gBryTE1zZH1F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text = 'muito obrigado'\n",
        "sentence = nltk.word_tokenize(text.lower(), language='portuguese')\n",
        "query = preprocess(sentence)\n",
        "sims = instance[query]  # A query observa na classe de similaridade."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M60_YkySdqa0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Mostramos os resultado à da frase perguntada na celula anterior \n",
        "print('Query:')\n",
        "print(text)\n",
        "for i in range(num_best):\n",
        "    print(\"\")\n",
        "    print('sim = %.4f' % sims[i][1])\n",
        "    print(\" \".join(sentences[sims[i][0]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VypFXL6_eDE3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('petróleo')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3j3bspM7eTeE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tensor_file = 'word_tensor.w2v'\n",
        "model.wv.save_word2vec_format(tensor_file)\n",
        "key_vectors = gensim.models.KeyedVectors.load_word2vec_format(tensor_file, binary=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QLn59GPXeWYY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words_id = []\n",
        "vectors = []\n",
        "for word_id in key_vectors.index2word[:10000]:\n",
        "    words_id.append(word_id)\n",
        "    vectors.append(key_vectors[word_id])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SS2gFMQUepCI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_tensorboard_files(tensor_filename, vectors, metadatos, colnames=None):\n",
        "\n",
        "    out_file_tsv      = tensor_filename + '_tensor.tsv'\n",
        "    out_file_tsv_meta = tensor_filename + '_metadata.tsv'\n",
        "    \n",
        "    with open(out_file_tsv, 'w',encoding='utf-8') as f:   \n",
        "        for vector in vectors:\n",
        "            vector_str = \"\\t\".join([str(x) for x in vector])\n",
        "            f.write( vector_str + '\\n')\n",
        "\n",
        "    with open(out_file_tsv_meta, 'w',encoding='utf-8') as f:\n",
        "        writer = csv.writer(f, delimiter='\\t')\n",
        "        if len(metadatos)>=2:\n",
        "            if colnames is None:\n",
        "                colnames = \"\\t\".join([str(i) for i in range(len(metadatos))])\n",
        "            writer.writerow(colnames)\n",
        "            for metadato in zip(*metadatos):\n",
        "                line = [str(x) for x in metadato]\n",
        "                writer.writerow(line)\n",
        "        else:\n",
        "            for metadato in metadatos[0]:\n",
        "                writer.writerow([metadato])\n",
        "            \n",
        "    print(\"Arquivo com o Tensor 2D foi salvado em: %s\" % out_file_tsv)\n",
        "    print(\"Arquivo com o Tensor de metadatos foi salvado em: %s\" % out_file_tsv_meta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OJjbsKD5ehEX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "metadatos=[words_id]\n",
        "create_tensorboard_files(tensor_filename=\"word\", \n",
        "                      vectors=vectors, \n",
        "                      metadatos=[words_id])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qm4l3Us2fAx6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Baixa os arquivos gerados e colocar em Tensoboard:\n"
      ]
    },
    {
      "metadata": {
        "id": "1DYtRM5Ef25U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('word_tensor.tsv')\n",
        "files.download('word_metadata.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}